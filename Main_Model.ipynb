{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TD14yuU60_y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pickle\n",
        "\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('CreditCardData.csv')\n",
        "\n",
        "df.drop([\"Transaction ID\"], axis=1, inplace=True)\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df[\"Amount\"] = df[\"Amount\"].str.replace(\"Â£\", \"\").astype(float)\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df.drop([\"Date\"], axis=1, inplace=True)\n",
        "df.drop([\"Month\", \"Year\"], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "X = df.drop(columns=['Fraud'])\n",
        "y = df['Fraud']\n",
        "\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "\n",
        "\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(32))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJVjLPiS63Y_",
        "outputId": "c9399663-3184-4bce-bea1-3a98ea2a3e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-6d424f83fdab>:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unique_classes = np.unique(y_train)\n",
        "print(\"Unique classes in y_train:\", unique_classes)\n",
        "\n",
        "\n",
        "if len(unique_classes) == 2:\n",
        "    class_mapping = {unique_classes[0]: 0, unique_classes[1]: 1}\n",
        "    y_train_mapped = np.vectorize(class_mapping.get)(y_train)\n",
        "    y_test_mapped = np.vectorize(class_mapping.get)(y_test)\n",
        "\n",
        "    # Computeed class weights using mapped classes\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_mapped), y=y_train_mapped)\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    history = model.fit(X_train_reshaped, y_train_mapped, epochs=20, batch_size=64,\n",
        "                        validation_split=0.2, verbose=2, class_weight=class_weight_dict)\n",
        "else:\n",
        "    print(\"Unexpected number of classes. Please check the labels.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSY9ooM463bV",
        "outputId": "c9df5780-48d6-4bff-ee31-12a8f6216d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique classes in y_train: [0 1]\n",
            "Epoch 1/20\n",
            "1000/1000 - 7s - 7ms/step - accuracy: 0.9334 - loss: 0.1271 - val_accuracy: 0.9309 - val_loss: 0.1335\n",
            "Epoch 2/20\n",
            "1000/1000 - 10s - 10ms/step - accuracy: 0.9333 - loss: 0.1218 - val_accuracy: 0.9421 - val_loss: 0.1117\n",
            "Epoch 3/20\n",
            "1000/1000 - 12s - 12ms/step - accuracy: 0.9359 - loss: 0.1153 - val_accuracy: 0.9414 - val_loss: 0.1032\n",
            "Epoch 4/20\n",
            "1000/1000 - 10s - 10ms/step - accuracy: 0.9362 - loss: 0.1126 - val_accuracy: 0.9398 - val_loss: 0.1057\n",
            "Epoch 5/20\n",
            "1000/1000 - 9s - 9ms/step - accuracy: 0.9380 - loss: 0.1110 - val_accuracy: 0.9446 - val_loss: 0.1092\n",
            "Epoch 6/20\n",
            "1000/1000 - 11s - 11ms/step - accuracy: 0.9359 - loss: 0.1086 - val_accuracy: 0.9464 - val_loss: 0.1113\n",
            "Epoch 7/20\n",
            "1000/1000 - 19s - 19ms/step - accuracy: 0.9391 - loss: 0.1047 - val_accuracy: 0.9462 - val_loss: 0.0947\n",
            "Epoch 8/20\n",
            "1000/1000 - 7s - 7ms/step - accuracy: 0.9380 - loss: 0.1032 - val_accuracy: 0.9376 - val_loss: 0.0895\n",
            "Epoch 9/20\n",
            "1000/1000 - 9s - 9ms/step - accuracy: 0.9375 - loss: 0.1036 - val_accuracy: 0.9490 - val_loss: 0.0904\n",
            "Epoch 10/20\n",
            "1000/1000 - 10s - 10ms/step - accuracy: 0.9388 - loss: 0.1006 - val_accuracy: 0.9317 - val_loss: 0.1111\n",
            "Epoch 11/20\n",
            "1000/1000 - 7s - 7ms/step - accuracy: 0.9368 - loss: 0.0995 - val_accuracy: 0.9434 - val_loss: 0.0903\n",
            "Epoch 12/20\n",
            "1000/1000 - 12s - 12ms/step - accuracy: 0.9389 - loss: 0.0977 - val_accuracy: 0.9421 - val_loss: 0.0970\n",
            "Epoch 13/20\n",
            "1000/1000 - 9s - 9ms/step - accuracy: 0.9390 - loss: 0.0986 - val_accuracy: 0.9402 - val_loss: 0.0913\n",
            "Epoch 14/20\n",
            "1000/1000 - 8s - 8ms/step - accuracy: 0.9400 - loss: 0.0926 - val_accuracy: 0.9457 - val_loss: 0.0902\n",
            "Epoch 15/20\n",
            "1000/1000 - 11s - 11ms/step - accuracy: 0.9393 - loss: 0.0952 - val_accuracy: 0.9454 - val_loss: 0.0890\n",
            "Epoch 16/20\n",
            "1000/1000 - 9s - 9ms/step - accuracy: 0.9429 - loss: 0.0924 - val_accuracy: 0.9446 - val_loss: 0.0944\n",
            "Epoch 17/20\n",
            "1000/1000 - 8s - 8ms/step - accuracy: 0.9410 - loss: 0.0929 - val_accuracy: 0.9432 - val_loss: 0.0930\n",
            "Epoch 18/20\n",
            "1000/1000 - 9s - 9ms/step - accuracy: 0.9419 - loss: 0.0906 - val_accuracy: 0.9359 - val_loss: 0.1009\n",
            "Epoch 19/20\n",
            "1000/1000 - 12s - 12ms/step - accuracy: 0.9416 - loss: 0.0911 - val_accuracy: 0.9415 - val_loss: 0.0859\n",
            "Epoch 20/20\n",
            "1000/1000 - 14s - 14ms/step - accuracy: 0.9402 - loss: 0.0913 - val_accuracy: 0.9479 - val_loss: 0.0823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('credit_card_fraud_preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "model.save('credit_card_fraud_model_lstm.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WahAcw_063fQ",
        "outputId": "94a0a59f-159b-4a50-b7c7-0f36005300be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9wlMXP363hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTJ6Ky5r63j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-V5IXRfK63mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WX3v7Wn63on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSeAJexH63rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hmk9dJ5Y63um"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}